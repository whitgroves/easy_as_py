{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The purpose of this exercise is to download an article from a Washington Post URL and use natural language processing (NLP) to summarize it in three sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize # Split text into sentences, and sentences into words.\n",
    "from nltk.corpus import stopwords # Used to filter out common words (\"and\", \"but\", \"I\", \"the\", etc.)\n",
    "from string import punctuation # Used to filter out punctuation.\n",
    "from collections import defaultdict # Dictionary that creates entries for new keys instead of throwing a KeyError.\n",
    "from heapq import nlargest # Returns the 'n' largest items from a list based on a method.\n",
    "\n",
    "# Uses ntlk to summarize a set of text based on word frequency.\n",
    "class FrequencySummarizer:\n",
    "    \n",
    "    def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "        # Words with a frequency term outside of the range between min_cut and max_cut will be ignored.\n",
    "        self._min_cut = min_cut\n",
    "        self._max_cut = max_cut\n",
    "        # Common words and symbols to be ignored for summarization.\n",
    "        self._stopwords = set(stopwords.words('english')+list(punctuation))\n",
    "    \n",
    "    # Accepts a list of sentences. \n",
    "    # Returns a dictionary of word:frequency pairs.\n",
    "    def _compute_frequencies(self, word_sent):\n",
    "        # Create a defaultdict (see import at top) with 0 as the default value.\n",
    "        freq = defaultdict(int)\n",
    "        # Loop through each word of each sentence to count their frequencies.\n",
    "        for sentence in word_sent:\n",
    "            for word in sentence:\n",
    "                if word not in self._stopwords:\n",
    "                    # Note: Using the regular dictionary would require checking if the key exists first.\n",
    "                    # because freq is a defaultdict, it will create an entry the first time a word is added.\n",
    "                    freq[word] += 1\n",
    "        # Normalize the frequencies so they fall between 0 and 1 by dividing the frequency of every word by\n",
    "        # the largest frequency in the dictionary.\n",
    "        max_n = float(max(freq.values()))\n",
    "        for word in list(freq.keys()):\n",
    "            freq[word] = freq[word]/max_n \n",
    "            # Frequencies outside of max_cut and min_cut are also filtered out in this step.\n",
    "            if freq[word] >= self._max_cut or freq[word] <= self._min_cut:\n",
    "                del freq[word]\n",
    "        # Return the dictionary of frequencies.\n",
    "        return freq\n",
    "    \n",
    "    # Accepts text and the number of sentences the summary should contain.\n",
    "    # Returns a summary of the text.\n",
    "    def summarize(self, text, n):\n",
    "        # Split the text into sentences (see import at top)\n",
    "        sentences = sent_tokenize(text)\n",
    "        # Sanity check to make sure the summary is less than the length of the article.\n",
    "        assert n <= len(sentences)\n",
    "        # Compile each word of each sentence into a single dictionary.\n",
    "        # For consistency, they're all converted to lowercase.\n",
    "        word_sent = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "        # Compute the word frequencies (see above method) and store in member variable _freq.\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        # Create a defaultdict to rank sentences by frequency.\n",
    "        rankings = defaultdict(int)\n",
    "        # First, enumerate each of the stentences for easier ranking.\n",
    "        for i,sentence in enumerate(word_sent):\n",
    "            # For each word in a sentence...\n",
    "            for word in sentence:\n",
    "                # ...if that word wasn't a stopword...\n",
    "                if word in self._freq:\n",
    "                    # ...add its frequency to the ranking.\n",
    "                    rankings[i] += self._freq[word]\n",
    "        # Fetch the indexes of the largest n sentences.\n",
    "        # nlargest (see import at top) is given ranking.get() to know which values to rank.\n",
    "        sentences_index = nlargest(n, rankings, key=rankings.get)\n",
    "        # Return a list containing the top sentences.\n",
    "        return [sentences[j] for j in sentences_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request # Downloads URLs.\n",
    "from bs4 import BeautifulSoup # Parses html pages in an easy-to-use way.\n",
    "\n",
    "# Accepts a URL for a Washington Post article.\n",
    "# Returns a title,body pair containing only text from the article.\n",
    "def get_text_from_wapo_url(url):\n",
    "    # Download the URL. Assumed to use utf-8 encoding.\n",
    "    page = urllib.request.urlopen(url).read().decode('utf8')\n",
    "    # Initialize a BeautifulSoup (see imports above) with url's text.\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    # WaPo wraps all of their articles in <article> tags, which is used\n",
    "    # to search for them here via BeautifulSoup.\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n",
    "#     print(text)\n",
    "    # Since other stuff is usually included between the <article> tags,\n",
    "    # another pass is necessary to filter to just article text. This is\n",
    "    # done by searching that text for <p> (paragraph) tags.\n",
    "#     soup2 = BeautifulSoup(text, 'html.parser')\n",
    "#     text = ' '.join(map(lambda p: p.text, soup2.find_all('p')))\n",
    "#     print(text)\n",
    "    #Return the title of the article and its text.\n",
    "    return soup.title.text, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.washingtonpost.com/opinions/global-opinions/after-a-mere-25-years-the-triumph-of-the-west-is-over/2016/12/01/deebe24c-b7f7-11e6-959c-172c82123976_story.html?hpid=hp_no-name_opinion-card-d%3Ahomepage%2Fstory\n",
      "And even as Europe tires of the sanctions imposed on Russia for its rape of Ukraine, President Obama’s much-touted “isolation” of Russia has ignominiously dissolved, as our secretary of state repeatedly goes cap in hand to Russia to beg for mercy in Syria.\n",
      "Read more here:   Fred Hiatt: The U.S. steps back from the world stage, and the consensus for leadership dissolves   Anne Applebaum: After Brussels, the West must reject dangerous isolationism   Jim Hoagland: Obama has ignored our European allies for too long   David Ignatius: What President Trump’s foreign policy will look like    \n",
      "Obama ordered retreat because he’s always felt the U.S. was not good enough for the world, too flawed to have earned the moral right to be the world hegemon.\n"
     ]
    }
   ],
   "source": [
    "someUrl = input()\n",
    "urlText = get_text_from_wapo_url(someUrl)\n",
    "fs = FrequencySummarizer()\n",
    "summary = fs.summarize(urlText[1], 3)\n",
    "for sentence in summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
