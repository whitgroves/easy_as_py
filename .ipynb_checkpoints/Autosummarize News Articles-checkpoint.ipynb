{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The purpose of this exercise is to download an article from a Washington Post URL and use natural language processing (NLP) to summarize it in three sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize # Split text into sentences, and sentences into words.\n",
    "from nltk.corpus import stopwords # Used to filter out common words (\"and\", \"but\", \"I\", \"the\", etc.)\n",
    "from string import punctuation # Used to filter out punctuation.\n",
    "from collections import defaultdict # Dictionary that creates entries for new keys instead of throwing a KeyError.\n",
    "from heapq import nlargest # Returns the 'n' largest items from a list based on a method.\n",
    "\n",
    "# Uses ntlk to summarize a set of text based on word frequency.\n",
    "class FrequencySummarizer:\n",
    "    \n",
    "    def __init__(self, min_cut=0.1, max_cut=0.9):\n",
    "        # Words with a frequency term outside of the range between min_cut and max_cut will be ignored.\n",
    "        self._min_cut = min_cut\n",
    "        self._max_cut = max_cut\n",
    "        # Common words and symbols to be ignored for summarization.\n",
    "        self._stopwords = set(stopwords.words('english')+list(punctuation))\n",
    "    \n",
    "    # Accepts a list of sentences. \n",
    "    # Returns a dictionary of word:frequency pairs.\n",
    "    def _compute_frequencies(self, word_sent):\n",
    "        # Create a defaultdict (see import at top) with 0 as the default value.\n",
    "        freq = defaultdict(int)\n",
    "        # Loop through each word of each sentence to count their frequencies.\n",
    "        for sentence in word_sent:\n",
    "            for word in sentence:\n",
    "                if word not in self._stopwords:\n",
    "                    # Note: Using the regular dictionary would require checking if the key exists first.\n",
    "                    # because freq is a defaultdict, it will create an entry the first time a word is added.\n",
    "                    freq[word] += 1\n",
    "        # Normalize the frequencies so they fall between 0 and 1 by dividing the frequency of every word by\n",
    "        # the largest frequency in the dictionary.\n",
    "        max_n = float(max(freq.values()))\n",
    "        for word in list(freq.keys()):\n",
    "            freq[word] = freq[word]/max_n \n",
    "            # Frequencies outside of max_cut and min_cut are also filtered out in this step.\n",
    "            if freq[word] >= self._max_cut or freq[word] <= self._min_cut:\n",
    "                del freq[word]\n",
    "        # Return the dictionary of frequencies.\n",
    "        return freq\n",
    "    \n",
    "    # Accepts text and the number of sentences the summary should contain.\n",
    "    # Returns a summary of the text.\n",
    "    def summarize(self, text, n):\n",
    "        # Split the text into sentences (see import at top)\n",
    "        sentences = sent_tokenize(text)\n",
    "        # Sanity check to make sure the summary is less than the length of the article.\n",
    "        assert n <= len(sentences)\n",
    "        # Compile each word of each sentence into a single dictionary.\n",
    "        # For consistency, they're all converted to lowercase.\n",
    "        word_sent = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "        # Compute the word frequencies (see above method) and store in member variable _freq.\n",
    "        self._freq = self._compute_frequencies(word_sent)\n",
    "        # Create a defaultdict to rank sentences by frequency.\n",
    "        rankings = defaultdict(int)\n",
    "        # First, enumerate each of the stentences for easier ranking.\n",
    "        for i,sentence in enumerate(word_sent):\n",
    "            # For each word in a sentence...\n",
    "            for word in sentence:\n",
    "                # ...if that word wasn't a stopword...\n",
    "                if word in self._freq:\n",
    "                    # ...add its frequency to the ranking.\n",
    "                    rankings[i] += self._freq[word]\n",
    "        # Fetch the indexes of the largest n sentences.\n",
    "        # nlargest (see import at top) is given ranking.get() to know which values to rank.\n",
    "        sentences_index = nlargest(n, rankings, key=rankings.get)\n",
    "        # Return a list containing the top sentences.\n",
    "        return [sentences[j] for j in sentences_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request # Downloads URLs.\n",
    "from bs4 import BeautifulSoup # Parses html pages in an easy-to-use way.\n",
    "\n",
    "# Accepts a URL for a Washington Post article.\n",
    "# Returns a title,body pair containing only text from the article.\n",
    "def get_text_from_wapo_url(url):\n",
    "    # Download the URL. Assumed to use utf-8 encoding.\n",
    "    page = urllib.request.urlopen(url).read().decode('utf8')\n",
    "    # Initialize a BeautifulSoup (see imports above) with url's text.\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    # WaPo wraps all of their articles in <article> tags, which is used\n",
    "    # to search for them here via BeautifulSoup.\n",
    "    text = ' '.join(map(lambda p: p.text, soup.find_all('article')))\n",
    "#     print(text)\n",
    "    # Since other stuff is usually included between the <article> tags,\n",
    "    # another pass is necessary to filter to just article text. This is\n",
    "    # done by searching that text for <p> (paragraph) tags.\n",
    "#     soup2 = BeautifulSoup(text, 'html.parser')\n",
    "#     text = ' '.join(map(lambda p: p.text, soup2.find_all('p')))\n",
    "#     print(text)\n",
    "    #Return the title of the article and its text.\n",
    "    return soup.title.text, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.washingtonpost.com/news/post-nation/wp/2016/11/26/clinton-campaign-will-participate-in-wisconsin-recount-with-an-eye-on-outside-interference-lawyer-says/?hpid=hp_hp-top-table-main_pn-recount-1234pm%3Ahomepage%2Fstory\n",
      "['In a Medium post, Clinton campaign lawyer Marc Elias said that the campaign had received “hundreds of messages, emails, and calls urging us to do something, anything, to investigate claims that the election results were hacked and altered in a way to disadvantage Secretary Clinton,” especially in Michigan, Wisconsin and Pennsylvania, where the “combined margin of victory for Donald Trump was merely 107,000 votes.” Elias said the campaign had “not uncovered any actionable evidence of hacking or outside attempts to alter the voting technology.” But because of the margin of victory — and because of the degree of apparent foreign interference during the campaign — Elias said that Clinton officials had “quietly taken a number of steps in the last two weeks to rule in or out any possibility of outside interference in the vote tally in these critical battleground states.”   [Trump calls recount efforts ‘sad,’ declares: ‘Nothing will change’]   He said that the Clinton campaign would participate in the Stein-initiated recount in Wisconsin by having representatives on the ground monitoring the count and having lawyers represent them in court if needed.', 'According to New York Magazine, the group found that Clinton “received 7 percent fewer votes in counties that relied on electronic-voting machines compared with counties that used optical scanners and paper ballots,” and that based on that “statistical analysis, Clinton may have been denied as many as 30,000 votes; she lost Wisconsin by 27,000.” J. Alex Halderman, one of the academics reportedly involved, later wrote on Medium that the deviations were “probably not” the result of a cyberattack but that “the only way to know whether a cyberattack changed the result is to closely examine the available physical evidence\\u200a — \\u200apaper ballots and voting equipment in critical states like Wisconsin, Michigan, and Pennsylvania.” Posting a link to a New York Times story about Clinton supporters calling for a recount, senior Trump adviser Kellyanne Conway said, “Look who ‘can’t accept the election results.’”   Look who \"can\\'t accept the election results\" Hillary Clinton Supporters Call for Vote Recount in Battleground States https://t.co/D4t6HbPUXG — Kellyanne Conway (@KellyannePolls) November 24, 2016   Elias’s post might fuel similar criticism.', 'In a statement Saturday, Trump said the recount was “just a way for Jill Stein, who received less than one\\xa0percent of the vote overall and wasn’t even on the ballot in many states, to fill her coffers with money.” “The people have spoken and the election is over,” the statement said, “and as Hillary Clinton herself said on election night,\\xa0in addition to her conceding by congratulating me, ‘We must accept this result and then look to the future.']\n"
     ]
    }
   ],
   "source": [
    "someUrl = input()\n",
    "urlText = get_text_from_wapo_url(someUrl)\n",
    "fs = FrequencySummarizer()\n",
    "summary = fs.summarize(urlText[1], 3)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
